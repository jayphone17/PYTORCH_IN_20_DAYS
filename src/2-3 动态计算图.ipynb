{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def0c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9451)\n",
      "tensor([[ 0.5027],\n",
      "        [-2.8926],\n",
      "        [ 1.4796],\n",
      "        [ 1.5360],\n",
      "        [ 0.1549],\n",
      "        [-1.4952],\n",
      "        [-0.6508],\n",
      "        [ 2.0322],\n",
      "        [ 5.1410],\n",
      "        [ 2.6279]])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch的计算图由节点和边组成，\n",
    "\n",
    "# 节点表示张量或者Function，\n",
    "\n",
    "# 边表示张量和Function之间的依赖关系。 \n",
    "\n",
    "# Pytorch中的计算图是动态图。这里的动态主要有两重含义。 \n",
    "\n",
    "# 第一层含义是：计算图的正向传播是立即执行的。\n",
    "# 无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，\n",
    "# 并立即执行正向传播得到计算结果。\n",
    "\n",
    "# 第二层含义是：计算图在反向传播后立即销毁。\n",
    "# 下次调用需要重新构建计算图。\n",
    "# 如果在程序中使 用了backward方法执行了反向传播，\n",
    "# 或者利用torch.autograd.grad方法计算了梯度，\n",
    "# 那么创建的 计算图会被立即销毁，释放存储空间，下次调用需要重新创建\n",
    "\n",
    "# 一、Brief Introduction of 动态计算图\n",
    "\n",
    "# 1. 正向传播是立即执行的\n",
    "\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b \n",
    "# Y_hat定义后正向传播被立即执行，与其后面的loss创建语句无关\n",
    "\n",
    "loss = torch.mean(torch.pow(Y_hat - Y,2))\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2392c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.9719)\n",
      "tensor([[ 5.5422],\n",
      "        [-1.3727],\n",
      "        [ 2.7483],\n",
      "        [-1.0296],\n",
      "        [ 4.8234],\n",
      "        [ 3.3021],\n",
      "        [ 2.9882],\n",
      "        [-4.5320],\n",
      "        [ 5.4670],\n",
      "        [ 0.1147]])\n"
     ]
    }
   ],
   "source": [
    "# 2. 计算图在反向传播后立即销毁\n",
    "\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b \n",
    "# Y_hat定义后正向传播被立即执行，与其后面的loss创建语句无关\n",
    "\n",
    "loss = torch.mean(torch.pow(Y_hat - Y,2))\n",
    "#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True\n",
    "loss.backward()\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fbae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二，计算图中的Function\n",
    "\n",
    "# 计算图中的张量我们已经比较熟悉了, \n",
    "# 计算图中的另外一种节点是Function, \n",
    "# 实际上就是 Pytorch 中各种对张量操作的函数。\n",
    "\n",
    "# 这些Function和我们Python中的函数有一个较大的区别，\n",
    "# 那就是它同时包括正向计算逻辑和反向传播的逻辑。 \n",
    "\n",
    "# 我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function.\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "#     正向传播逻辑，可以用ctx存储一些值，以供反向传播使用\n",
    "    @staticmethod\n",
    "    def forward(ctx,input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    \n",
    "#     反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca059045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000]])\n",
      "tensor([[4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]], requires_grad=True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])\n",
    "Y = torch.tensor([[2.0,3.0]])\n",
    "\n",
    "relu = MyReLU.apply\n",
    "Y_hat = relu(X@w.t() + b)\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d4abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MyReLUBackward object at 0x7fdd9f274ad0>\n"
     ]
    }
   ],
   "source": [
    "# Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward \n",
    "print(Y_hat.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922892bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三、 计算图与反向传播\n",
    "\n",
    "import torch \n",
    "x = torch.tensor(3.0,requires_grad=True) \n",
    "y1 = x + 1 \n",
    "y2 = 2*x \n",
    "loss = (y1-y2)**2\n",
    "loss.backward()\n",
    "\n",
    "# loss.backward()语句调用后，依次发生以下计算过程。\n",
    "# 1，loss自己的grad梯度赋值为1，即对自身的梯度为1。 \n",
    "\n",
    "# 2，loss根据其自身梯度以及关联的backward方法，\n",
    "# 计算出其对应的自变量即y1和y2的梯度，\n",
    "# 将该值赋值到y1.grad和y2.grad。 \n",
    "\n",
    "# 3，y2和y1根据其自身梯度以及关联的backward方法, \n",
    "# 分别计算出其对应的自变量x的梯度， \n",
    "# x.grad将其收到的多个梯度值累加。 \n",
    "\n",
    "# （注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述） \n",
    "# 正因为求导链式法则衍生的梯度累加规则，\n",
    "# 张量的grad梯度不会自动清零，在需要的时候需要手动置零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de593706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad: None\n",
      "y1.grad: None\n",
      "y2.grad: None\n",
      "tensor(4.)\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/lnzg195x2k52jp34khj_y9sh0000gn/T/ipykernel_3722/2407673036.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"loss.grad:\", loss.grad)\n",
      "/var/folders/vl/lnzg195x2k52jp34khj_y9sh0000gn/T/ipykernel_3722/2407673036.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"y1.grad:\", y1.grad)\n",
      "/var/folders/vl/lnzg195x2k52jp34khj_y9sh0000gn/T/ipykernel_3722/2407673036.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"y2.grad:\", y2.grad)\n"
     ]
    }
   ],
   "source": [
    "# 四、 叶子节点和非叶子节点\n",
    "\n",
    "import torch \n",
    "x = torch.tensor(3.0,requires_grad=True) \n",
    "y1 = x + 1 \n",
    "y2 = 2*x \n",
    "loss = (y1-y2)**2\n",
    "loss.backward() \n",
    "\n",
    "print(\"loss.grad:\", loss.grad) \n",
    "print(\"y1.grad:\", y1.grad) \n",
    "print(\"y2.grad:\", y2.grad) \n",
    "print(x.grad)\n",
    "\n",
    "print(x.is_leaf) \n",
    "print(y1.is_leaf) \n",
    "print(y2.is_leaf) \n",
    "print(loss.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910395a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2 grad:  tensor(4.)\n",
      "y1 grad:  tensor(-4.)\n",
      "loss_grad : tensor(1.)\n",
      "x_grad:  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 执行上面代码，\n",
    "# 我们会发现 loss.grad并不是我们期望的1,而是 None。 \n",
    "# 类似地 y1.grad 以及 y2.grad也是 None. \n",
    "# 这是为什么呢？\n",
    "\n",
    "# 这是由于它们不是叶子节点张量。 \n",
    "# 在反向传播过程中，只有 is_leaf=True 的叶子节点，\n",
    "# 需要求导的张量的导数结果才会被最后保留下来！！！！！\n",
    "\n",
    "# 那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件：\n",
    "\n",
    "# 1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。 \n",
    "# 2，叶子节点张量的 requires_grad属性必须为True. \n",
    "\n",
    "# Pytorch设计这样的规则主要是为了节约内存或者显存空间，\n",
    "\n",
    "# 因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "\n",
    "# 所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，\n",
    "\n",
    "# 但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。 \n",
    "\n",
    "# 如果需要保留中间计算结果的梯度到grad属性中，\n",
    "\n",
    "# 可以使用 retain_grad方法。 \n",
    "\n",
    "# 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。\n",
    "\n",
    "import torch \n",
    "x = torch.tensor(3.0,requires_grad=True) \n",
    "y1 = x + 1 \n",
    "y2 = 2*x \n",
    "loss = (y1-y2)**2\n",
    "\n",
    "y1.register_hook(lambda grad: print('y1 grad: ', grad))\n",
    "y2.register_hook(lambda grad:print ('y2 grad: ',grad))\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward() \n",
    "\n",
    "print(\"loss_grad :\", loss.grad)\n",
    "print(\"x_grad: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22379b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 五、 TensorBoard Visualize\n",
    "\n",
    "# 利用torch.utils.tensorboard将计算图到处到TensorBoard可视化\n",
    "\n",
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2,1))\n",
    "        self.b = nn.Parameter(torch.randn(1,1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = x@self.w + self.b\n",
    "        return y\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ada28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('../data/tensorboard')\n",
    "writer.add_graph(net, input_to_model = torch.rand(10,2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016442cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32da60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d491417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b245c6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3728), started 0:03:23 ago. (Use '!kill 3728' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6d4a23ca29c3300e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6d4a23ca29c3300e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#在tensorboard中查看模型 \n",
    "notebook.start(\"--logdir ../data/tensorboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
