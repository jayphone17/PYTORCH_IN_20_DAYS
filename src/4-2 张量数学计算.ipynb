{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6f011d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  8.],\n",
       "        [ 4., 12.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量数学运算主要有：标量运算，向量运算，矩阵运算。\n",
    "# 另外还有张量运算的广播机制。\n",
    "\n",
    "# 一、 标量运算\n",
    "\n",
    "# 张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。\n",
    "# 加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。\n",
    "# 标量运算符的特点是对张量实施逐元素运算。\n",
    "# 有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([[1.0,2], [-3, 4.0]])\n",
    "b = torch.tensor([[9.0,6], [7.0,8.0]])\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1dd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.,  -4.],\n",
       "        [-10.,  -4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f304514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.,  12.],\n",
       "        [-21.,  32.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fcae2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1111,  0.3333],\n",
       "        [-0.4286,  0.5000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057835ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3bd1b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4142],\n",
       "        [   nan, 2.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b252283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [-0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a%3 #求模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd16188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JayphoneLin\\anaconda3\\envs\\pytorch_study\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.],\n",
       "        [-1.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#地板除法!!!!!\n",
    "\n",
    "a//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16cb4034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>=2 # torch.ge(a,2)  #ge: greater_equal缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1f50ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>=2)&(a<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e02e55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>=2)|(a<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a8bf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==5 #torch.eq(a,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ec6599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4142],\n",
       "        [   nan, 2.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebdb2048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 21.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0,8.0])\n",
    "b = torch.tensor([5.0,6.0])\n",
    "c = torch.tensor([6.0,7.0])\n",
    "\n",
    "d = a+b+c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbc1c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 8.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(a,b))\n",
    "#每一列最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "828b21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de0d4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3., -3.])\n",
      "tensor([ 2., -3.])\n",
      "tensor([ 3., -2.])\n",
      "tensor([ 2., -2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.6,-2.7])\n",
    "\n",
    "print(torch.round(x)) #保留整数部分，四舍五入\n",
    "print(torch.floor(x)) #保留整数部分，向下归整\n",
    "print(torch.ceil(x))  #保留整数部分，向上归整\n",
    "print(torch.trunc(x)) #保留整数部分，向0归整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a369f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6000, -0.7000])\n",
      "tensor([0.6000, 1.3000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.6,-2.7])\n",
    "print(torch.fmod(x,2)) #作除法取余数 \n",
    "print(torch.remainder(x,2)) #作除法取剩余的部分，结果恒正！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad39b238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])\n",
      "tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])\n"
     ]
    }
   ],
   "source": [
    "# 幅值裁剪\n",
    "x = torch.tensor([0.9,-0.8,100.0,-20.0,0.7])\n",
    "y = torch.clamp(x,min=-1,max = 1)\n",
    "z = torch.clamp(x,max = 1)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eac6e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.)\n",
      "tensor(5.)\n",
      "tensor(9.)\n",
      "tensor(1.)\n",
      "tensor(362880.)\n",
      "tensor(2.7386)\n",
      "tensor(7.5000)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "# 二，向量运算\n",
    "# 向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。\n",
    "\n",
    "#统计值\n",
    "\n",
    "a = torch.arange(1,10).float()\n",
    "print(torch.sum(a)) \n",
    "print(torch.mean(a)) #平均值\n",
    "print(torch.max(a))\n",
    "print(torch.min(a))\n",
    "print(torch.prod(a)) #累乘\n",
    "print(torch.std(a))  #标准差\n",
    "print(torch.var(a))  #方差\n",
    "print(torch.median(a)) #中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4997a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "torch.return_types.max(\n",
      "values=tensor([7., 8., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 6., 9.]),\n",
      "indices=tensor([2, 2, 2]))\n"
     ]
    }
   ],
   "source": [
    "#指定维度计算统计值\n",
    "\n",
    "b = a.view(3,3)\n",
    "print(b)\n",
    "print(torch.max(b,dim = 0))\n",
    "print(torch.max(b,dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32c6c880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 1,  3,  6, 10, 15, 21, 28, 36, 45])\n",
      "tensor([     1,      2,      6,     24,    120,    720,   5040,  40320, 362880])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.return_types.cummin(\n",
      "values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "#cum扫描\n",
    "a = torch.arange(1,10)\n",
    "print(a)\n",
    "\n",
    "print(torch.cumsum(a,0)) #每一项都是前面项之和\n",
    "print(torch.cumprod(a,0)) #累乘\n",
    "print(torch.cummax(a,0).values) #值\n",
    "print(torch.cummax(a,0).indices) #下标\n",
    "print(torch.cummin(a,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "346c896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[9., 7., 8.],\n",
      "        [5., 6., 4.]]),\n",
      "indices=tensor([[0, 0, 0],\n",
      "        [2, 2, 2]])) \n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[9., 8.],\n",
      "        [3., 2.],\n",
      "        [6., 5.]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [1, 2],\n",
      "        [1, 0]])) \n",
      "\n",
      "torch.return_types.sort(\n",
      "values=tensor([[7., 8., 9.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 5., 6.]]),\n",
      "indices=tensor([[1, 2, 0],\n",
      "        [0, 2, 1],\n",
      "        [2, 0, 1]])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#torch.sort和torch.topk可以对张量排序\n",
    "a = torch.tensor([[9,7,8],[1,3,2],[5,6,4]]).float()\n",
    "\n",
    "print(torch.topk(a,2,dim = 0),\"\\n\")\n",
    "\n",
    "print(torch.topk(a,2,dim = 1),\"\\n\")\n",
    "\n",
    "print(torch.sort(a,dim = 1),\"\\n\")\n",
    "\n",
    "#利用torch.topk可以在Pytorch中实现KNN算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b16dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 三，矩阵运算\n",
    "# 矩阵必须是二维的。\n",
    "# 类似torch.tensor([1,2,3])这样的不是矩阵。\n",
    "# 矩阵运算包括：\n",
    "# 1. 矩阵乘法，\n",
    "# 2. 矩阵转置，\n",
    "# 3. 矩阵逆，\n",
    "# 4. 矩阵求迹，\n",
    "# 5. 矩阵范数，\n",
    "# 6. 矩阵行列式，\n",
    "# 7. 矩阵求特征值，\n",
    "# 8. 矩阵分解等运算。\n",
    "\n",
    "#矩阵乘法\n",
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[2,0],[0,2]])\n",
    "print(a@b)  #等价于torch.matmul(a,b) 或 torch.mm(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a32ee7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵转置\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(a)\n",
    "print(a.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b49f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵逆，必须为浮点类型\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.inverse(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fb6e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "#矩阵求trace\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "# 迹就是矩阵主对角线元素之和。\n",
    "print(torch.trace(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8786476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4772)\n"
     ]
    }
   ],
   "source": [
    "#矩阵求范数\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1098a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在数值计算中计算矩阵的算法中常常要判断算法的解是否收敛\n",
    "# 这时最准确的方法是判断矩阵的最大特征值\n",
    "# 但是矩阵的特征值得计算相对麻烦\n",
    "# 所以可以近似的用范数代替\n",
    "# 但是不够准确\n",
    "# 但是很高效\n",
    "# 理论上讲范数的概念属于赋范线性空间，\n",
    "# 最重要的作用是诱导出距离，进而还可以研究收敛性。\n",
    "# 对于矩阵而言没必要考虑范数的区别，\n",
    "# 因为有限维空间的范数都等价(Minkowski定理)，\n",
    "# 实际应用当中根据使用的难易程度来选取范数。\n",
    "# 其中理论性质最好的是2-范数，因为它可以由内积来诱导，\n",
    "# 同时和谱有着密切关联，所以常用来进行理论分析。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c4aef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0000)\n"
     ]
    }
   ],
   "source": [
    "#矩阵行列式\n",
    "a = torch.tensor([[1.0,2],[3,4]])\n",
    "print(torch.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec36a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[ 2.5000,  2.7839],\n",
      "        [ 2.5000, -2.7839]]),\n",
      "eigenvectors=tensor([[ 0.2535, -0.4706],\n",
      "        [ 0.8452,  0.0000]]))\n"
     ]
    }
   ],
   "source": [
    "#矩阵特征值和特征向量\n",
    "a = torch.tensor([[1.0,2],[-5,4]],dtype = torch.float)\n",
    "print(torch.eig(a,eigenvectors=True))\n",
    "\n",
    "#两个特征值分别是 -2.5+2.7839j, 2.5-2.7839j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69006f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3162, -0.9487],\n",
      "        [-0.9487,  0.3162]]) \n",
      "\n",
      "tensor([[-3.1623, -4.4272],\n",
      "        [ 0.0000, -0.6325]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r\n",
    "#QR分解实际上是对矩阵a实施Schmidt正交化得到q\n",
    "\n",
    "a  = torch.tensor([[1.0,2.0],[3.0,4.0]])\n",
    "q,r = torch.qr(a)\n",
    "print(q,\"\\n\")\n",
    "print(r,\"\\n\")\n",
    "print(q@r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffc5c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2298,  0.8835],\n",
      "        [-0.5247,  0.2408],\n",
      "        [-0.8196, -0.4019]]) \n",
      "\n",
      "tensor([9.5255, 0.5143]) \n",
      "\n",
      "tensor([[-0.6196, -0.7849],\n",
      "        [-0.7849,  0.6196]]) \n",
      "\n",
      "tensor([[1.0000, 2.0000],\n",
      "        [3.0000, 4.0000],\n",
      "        [5.0000, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵svd分解\n",
    "#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积\n",
    "#svd常用于矩阵压缩和降维\n",
    "a=torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "\n",
    "u,s,v = torch.svd(a)\n",
    "\n",
    "print(u,\"\\n\")\n",
    "print(s,\"\\n\")\n",
    "print(v,\"\\n\")\n",
    "\n",
    "print(u@torch.diag(s)@v.t())\n",
    "\n",
    "#利用svd分解可以在Pytorch中实现主成分分析降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c69b83b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 四，广播机制\n",
    "# Pytorch的广播规则和numpy是一样的:\n",
    "\n",
    "# 1、如果张量的维度不同，将维度较小的张量进行 <扩展>，直到两个张量的维度都一样。\n",
    "\n",
    "# 2、如果两个张量在某个维度上的长度是相同的，\n",
    "# 或者其中一个张量在该维度上的长度为1，\n",
    "# 那么我们就说这两个张量在该维度上是 <相容> 的。\n",
    "\n",
    "# 3、如果两个张量在所有维度上都是相容的，它们就能使用 <广播>。\n",
    "\n",
    "# 4、广播之后，每个维度的长度将取两个张量在该维度长度的 <较大值>。\n",
    "\n",
    "# 5、在任何一个维度上，如果一个张量的长度为1，\n",
    "# 另一个张量长度大于1，那么在该维度上，\n",
    "# 就好像是对第一个张量进行了复制。\n",
    "\n",
    "# torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。\n",
    "\n",
    "a = torch.tensor([1,2,3]) #维度不够，剩下维度都是[1,2,3], 看下面代码。\n",
    "b = torch.tensor([[0,0,0],[1,1,1],[2,2,2]])\n",
    "print(b + a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f86b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]]) \n",
      "\n",
      "tensor([[0, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [2, 2, 2]]) \n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a_broad,b_broad = torch.broadcast_tensors(a,b)\n",
    "print(a_broad,\"\\n\")\n",
    "print(b_broad,\"\\n\")\n",
    "print(a_broad + b_broad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d4fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
