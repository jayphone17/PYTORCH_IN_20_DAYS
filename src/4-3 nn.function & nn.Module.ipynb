{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f3cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a31259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一，nn.functional 和 nn.Module\n",
    "# 前面学习Pytorch的张量的结构操作和数学运算中的一些常用API。\n",
    "\n",
    "# 利用这些张量的API可以构建出神经网络相关的组件 (如激活函数，模型层，损失函数)。\n",
    "\n",
    "# Pytorch和神经网络相关的功能组件大多都封装在 torch.nn 模块下。\n",
    "\n",
    "# 这些功能组件的绝大部分既有函数形式实现，也有类形式实现。\n",
    "\n",
    "# 其中nn.functional(一般引入后改名为F)有各种功能组件的函数实现。例如：\n",
    "\n",
    "# (激活函数)\n",
    "# F.relu\n",
    "# F.sigmoid\n",
    "# F.tanh\n",
    "# F.softmax\n",
    "\n",
    "# (模型层)\n",
    "# F.linear\n",
    "# F.conv2d\n",
    "# F.max_pool2d\n",
    "# F.dropout2d\n",
    "# F.embedding\n",
    "\n",
    "# (损失函数)\n",
    "# F.binary_cross_entropy\n",
    "# F.mse_loss\n",
    "# F.cross_entropy\n",
    "\n",
    "# 为了便于对参数进行管理，一般通过继承 nn.Module转换成为类的实现形式，\n",
    "# 并直接封装在 nn 模块下。例如：\n",
    "\n",
    "# (激活函数)\n",
    "# nn.ReLU\n",
    "# nn.Sigmoid\n",
    "# nn.Tanh\n",
    "# nn.Softmax\n",
    "\n",
    "# (模型层)\n",
    "# nn.Linear\n",
    "# nn.Conv2d\n",
    "# nn.MaxPool2d\n",
    "# nn.Dropout2d\n",
    "# nn.Embedding\n",
    "\n",
    "# (损失函数)\n",
    "# nn.BCELoss\n",
    "# nn.MSELoss\n",
    "# nn.CrossEntropyLoss\n",
    "# 实际上nn.Module除了可以管理其引用的各种参数，\n",
    "# 还可以管理其引用的子模块，功能十分强大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb97deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二，使用nn.Module来管理参数\n",
    "# 在Pytorch中，模型的参数是需要被优化器训练的，\n",
    "\n",
    "# 因此，通常要设置参数为 requires_grad = True 的张量。\n",
    "\n",
    "# 同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。\n",
    "\n",
    "# Pytorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数。\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0b83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2490,  0.2193],\n",
      "        [-0.8261,  0.3391]], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# nn.Parameter 具有 requires_grad = True 属性\n",
    "w = nn.Parameter(torch.randn(2,2))\n",
    "print(w)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460e0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterList(\n",
      "    (0): Parameter containing: [torch.FloatTensor of size 8x1]\n",
      "    (1): Parameter containing: [torch.FloatTensor of size 8x2]\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# nn.ParameterList 可以将多个nn.Parameter组成一个列表\n",
    "params_list = nn.ParameterList([nn.Parameter(torch.rand(8,i)) for i in range(1,3)])\n",
    "print(params_list)\n",
    "print(params_list[0].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638319a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterDict(\n",
      "    (a): Parameter containing: [torch.FloatTensor of size 2x2]\n",
      "    (b): Parameter containing: [torch.FloatTensor of size 2]\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# nn.ParameterDict 可以将多个nn.Parameter组成一个 <字典>\n",
    "\n",
    "params_dict = nn.ParameterDict({\"a\":nn.Parameter(torch.rand(2,2)),\n",
    "                               \"b\":nn.Parameter(torch.zeros(2))})\n",
    "print(params_dict)\n",
    "print(params_dict[\"a\"].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8331fe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2490,  0.2193],\n",
      "        [-0.8261,  0.3391]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.7711],\n",
      "        [0.1007],\n",
      "        [0.3031],\n",
      "        [0.3746],\n",
      "        [0.9746],\n",
      "        [0.4041],\n",
      "        [0.8828],\n",
      "        [0.9737]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.0241, 0.7066],\n",
      "        [0.2874, 0.5210],\n",
      "        [0.5923, 0.1408],\n",
      "        [0.8916, 0.3496],\n",
      "        [0.3920, 0.9891],\n",
      "        [0.7867, 0.5447],\n",
      "        [0.7750, 0.0087],\n",
      "        [0.5675, 0.7783]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.6958, 0.5185],\n",
      "        [0.1712, 0.3046]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True) \n",
      "\n",
      "number of Parameters = 5\n"
     ]
    }
   ],
   "source": [
    "# 可以用Module将参数们管理起来\n",
    "# module.parameters()返回一个生成器，包括其结构下的所有parameters\n",
    "\n",
    "module = nn.Module()\n",
    "module.w = w\n",
    "module.params_list = params_list\n",
    "module.params_dict = params_dict\n",
    "\n",
    "num_param = 0\n",
    "\n",
    "for param in module.parameters():\n",
    "    print(param,\"\\n\")\n",
    "    num_param = num_param + 1\n",
    "print(\"number of Parameters =\",num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f639c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实践当中，一般通过继承nn.Module来构建模块类，\n",
    "# 并将所有含有需要学习的参数的部分放在构造函数中。\n",
    "\n",
    "#以下范例为Pytorch中nn.Linear的源码的简化版本\n",
    "#可以看到它将需要学习的参数放在了__init__构造函数中，\n",
    "# 并在forward中调用F.linear函数来实现计算逻辑。\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b2df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三，使用nn.Module来管理子模块\n",
    "\n",
    "# 一般情况下，我们都很少直接使用 nn.Parameter来定义参数构建模型，\n",
    "# 而是通过一些拼装一些常用的模型层来构造模型。\n",
    "\n",
    "# 这些模型层也是继承自nn.Module的对象,本身也包括参数，属于我们要定义的模块的子模块。\n",
    "\n",
    "# nn.Module提供了一些方法可以管理这些子模块。\n",
    "\n",
    "# children() 方法: 返回生成器，包括模块下的所有子模块。\n",
    "    \n",
    "# named_children()方法：返回一个生成器，包括模块下的所有子模块，以及它们的名字。\n",
    "\n",
    "# modules()方法：返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。\n",
    "\n",
    "# named_modules()方法：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，\n",
    "# 包括模块本身。其中chidren()方法和named_children()方法较多使用。\n",
    "\n",
    "# modules()方法和named_modules()方法较少使用，\n",
    "# 其功能可以通过多个named_children()的嵌套使用实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94020db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = 10000,embedding_dim = 3,padding_idx = 1)\n",
    "        # embedding用来降维，原理是矩阵乘法\n",
    "        # 在CNN中可以理解为特殊全连接层操作。\n",
    "        \n",
    "        self.conv = nn.Sequential()\n",
    "        \n",
    "        self.conv.add_module(\"conv_1\",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))\n",
    "        self.conv.add_module(\"pool_1\",nn.MaxPool1d(kernel_size = 2))\n",
    "        \n",
    "        self.conv.add_module(\"relu_1\",nn.ReLU())\n",
    "        \n",
    "        self.conv.add_module(\"conv_2\",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))\n",
    "        self.conv.add_module(\"pool_2\",nn.MaxPool1d(kernel_size = 2))\n",
    "        \n",
    "        self.conv.add_module(\"relu_2\",nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        \n",
    "        self.dense.add_module(\"flatten\",nn.Flatten())\n",
    "        self.dense.add_module(\"linear\",nn.Linear(6144,1))\n",
    "        self.dense.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38db121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10000, 3, padding_idx=1) \n",
      "\n",
      "Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ") \n",
      "\n",
      "Sequential(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ") \n",
      "\n",
      "child number 3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for child in net.children():\n",
    "    i+=1\n",
    "    print(child,\"\\n\")\n",
    "print(\"child number\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c144eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding : Embedding(10000, 3, padding_idx=1) \n",
      "\n",
      "conv : Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ") \n",
      "\n",
      "dense : Sequential(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ") \n",
      "\n",
      "child number 3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for name,child in net.named_children():\n",
    "    i+=1\n",
    "    print(name,\":\",child,\"\\n\")\n",
    "print(\"child number\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78c0c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(10000, 3, padding_idx=1)\n",
      "  (conv): Sequential(\n",
      "    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (relu_1): ReLU()\n",
      "    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (relu_2): ReLU()\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Embedding(10000, 3, padding_idx=1)\n",
      "Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ")\n",
      "Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "ReLU()\n",
      "Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Flatten(start_dim=1, end_dim=-1)\n",
      "Linear(in_features=6144, out_features=1, bias=True)\n",
      "Sigmoid()\n",
      "module number: 13\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for module in net.modules():\n",
    "    i+=1\n",
    "    print(module)\n",
    "print(\"module number:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fe7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': Embedding(10000, 3, padding_idx=1), 'conv': Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      "), 'dense': Sequential(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(10000, 3, padding_idx=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面我们通过named_children方法找到embedding层，\n",
    "# 并将其参数设置为不可训练(相当于冻结embedding层)\n",
    "\n",
    "children_dict = {name:module for name,module in net.named_children()}\n",
    "\n",
    "print(children_dict)\n",
    "embedding = children_dict[\"embedding\"]\n",
    "embedding.requires_grad_(False) #冻结其参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c908379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "#可以看到其第一层的参数已经不可以被训练了。\n",
    "for param in embedding.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df92ba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1               [-1, 200, 3]          30,000\n",
      "            Conv1d-2              [-1, 16, 196]             256\n",
      "         MaxPool1d-3               [-1, 16, 98]               0\n",
      "              ReLU-4               [-1, 16, 98]               0\n",
      "            Conv1d-5              [-1, 128, 97]           4,224\n",
      "         MaxPool1d-6              [-1, 128, 48]               0\n",
      "              ReLU-7              [-1, 128, 48]               0\n",
      "           Flatten-8                 [-1, 6144]               0\n",
      "            Linear-9                    [-1, 1]           6,145\n",
      "          Sigmoid-10                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 40,625\n",
      "Trainable params: 10,625\n",
      "Non-trainable params: 30,000\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.000763\n",
      "Forward/backward pass size (MB): 0.287796\n",
      "Params size (MB): 0.154972\n",
      "Estimated Total Size (MB): 0.443531\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import summary\n",
    "summary(net,input_shape = (200,),input_dtype = torch.LongTensor)\n",
    "# 不可训练，参数数量增加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ccd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
